{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check wheter expected interaction reward matches with environment reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from Environment import Environment\n",
    "from Model.Evaluator.GraphEvaluator import GraphEvaluator\n",
    "from Model.Evaluator.Baseline import Baseline\n",
    "from Model.Evaluator.OneStepEvaluator import OneStepEvaluator\n",
    "\n",
    "from Model.Product import *\n",
    "import numpy as np\n",
    "\n",
    "# ==== PARAMETERS TO CHANGE ====\n",
    "config_path = \"./Configs/config1.json\"\n",
    "RANDOM_ARM = False\n",
    "arm = [1, 2, 0, 1, 0]\n",
    "n_experiments = 500\n",
    "# ==============================\n",
    "\n",
    "f = open(config_path)\n",
    "config = json.load(f)\n",
    "f.close()\n",
    "\n",
    "env = Environment(config_path=config_path)\n",
    "marginsPerPrice = config[\"margins\"]\n",
    "\n",
    "if RANDOM_ARM:\n",
    "    arm = np.floor(np.random.rand(len(marginsPerPrice)) * len(marginsPerPrice[0]))\n",
    "    arm = np.array(arm.tolist(), dtype=int)\n",
    "\n",
    "margins = [marginsPerPrice[i][arm[i]] for i in range(0,len(arm))]\n",
    "# print(margins)\n",
    "obtained_margins = []\n",
    "\n",
    "conf_classes = config[\"classes\"]\n",
    "for uc in conf_classes:\n",
    "    armConvRates = [uc[\"conversionRates\"][i][arm[i]] for i in range(0,len(arm))]\n",
    "    productList = [Product(int(key), uc[\"secondary\"][key]) for key in uc[\"secondary\"]]\n",
    "    eval = GraphEvaluator(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"unitsShape\"], convert_units=True, verbose=False)\n",
    "    baseline = Baseline(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"unitsShape\"], convert_units=True, verbose=False)\n",
    "    oneStep = OneStepEvaluator(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"unitsShape\"], verbose=False)\n",
    "\n",
    "env.setPriceLevels(arm)\n",
    "for i in range(0,n_experiments):\n",
    "  inters = env.round()\n",
    "  total = 0\n",
    "  for inter in inters:\n",
    "    total += inter.linearizeMargin(marginsPerPrice)\n",
    "    obtained_margins.append(inter.linearizeMargin(marginsPerPrice))\n",
    "  total = total / len(inters)\n",
    "  # obtained_margins.append(total)\n",
    "\n",
    "print(\"SINGLE ITERATION/SESSION REWARDS FOR CONFIG {}:\".format(arm))\n",
    "print(\"   - [EMPIRICAL] Mean reward ({} experiments from env): {}\".format(n_experiments, np.array(obtained_margins).mean()))\n",
    "print(\"   - [THEORETICAL] Graph expected reward: {}\".format(eval.computeMargin()))\n",
    "print(\"   - [THEORETICAL] Baseline expected reward: {}\".format(baseline.computeMargin()))\n",
    "print(\"   - [THEORETICAL] One-Step expected reward: {}\".format(oneStep.computeMargin()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the best arm for each class by brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from Environment import Environment\n",
    "import numpy as np\n",
    "from Learner.BruteForce import *\n",
    "from Model.UserClass import *\n",
    "from Model.Product import *\n",
    "from Model.GraphProbabilities import *\n",
    "from Model.Evaluator.GraphEvaluator import GraphEvaluator\n",
    "from Model.Evaluator.Baseline import Baseline\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "# ==== PARAMETERS TO CHANGE ====\n",
    "CONFIG_PATH = './Configs/configuration4.json'\n",
    "# ==============================\n",
    "\n",
    "\n",
    "f = open(CONFIG_PATH)\n",
    "config = json.load(f)\n",
    "f.close()\n",
    "\n",
    "opt_arms = []\n",
    "opt_margins = []\n",
    "daily_users = []\n",
    "print(\"Starting the analysis ...\\n\")\n",
    "for k in range(0, len(config[\"classes\"])):\n",
    "    uc = config[\"classes\"][k]\n",
    "\n",
    "    productList = [Product(int(key), uc[\"secondary\"][key]) for key in uc[\"secondary\"]]\n",
    "\n",
    "    conversionRateLevels = uc[\"conversionRates\"]\n",
    "    marginsPerPrice = config[\"margins\"]\n",
    "    click_prob = np.array(uc[\"clickProbability\"])\n",
    "    lambda_p = uc[\"lambda\"]\n",
    "    alphas = uc[\"alphas\"]\n",
    "    units_mean = uc[\"unitsShape\"]\n",
    "    # Early transform for efficiency reason\n",
    "    actual_means = []\n",
    "    for i in range(0,len(units_mean)):\n",
    "        empiric_mean = np.ceil(np.random.gamma(units_mean[i], 1, size=1000000)).mean()\n",
    "        actual_means.append(int(empiric_mean*100) / 100)\n",
    "    units_mean = actual_means\n",
    "\n",
    "    daily_users.append(uc[\"usersMean\"])\n",
    "    num_prices = len(conversionRateLevels[0])\n",
    "    num_prods = len(alphas)\n",
    "\n",
    "    print(\"Brute forcing class {}\".format(k))\n",
    "    bf = BruteForce(num_prices=num_prices, num_products=num_prods)\n",
    "    for i in tqdm(range(0, num_prices**num_prods)):\n",
    "        pulledArm = bf.pull_arm()\n",
    "        margins = []\n",
    "        convRates = []\n",
    "        for k in range(0,len(pulledArm)):\n",
    "            margins.append(marginsPerPrice[k][pulledArm[k]])\n",
    "            convRates.append(conversionRateLevels[k][pulledArm[k]])\n",
    "\n",
    "        price_configuration_margin = 0\n",
    "        eval = GraphEvaluator(products_list=productList, click_prob_matrix=click_prob, lambda_prob=lambda_p, conversion_rates=convRates,\n",
    "                        alphas=alphas, margins=margins, units_mean=units_mean, convert_units=False, verbose=False)\n",
    "        eval2 = Baseline(products_list=productList, click_prob_matrix=click_prob, lambda_prob=lambda_p, conversion_rates=convRates,\n",
    "                        alphas=alphas, margins=margins, units_mean=units_mean, convert_units=False, verbose=False)\n",
    "\n",
    "        overall_margin = eval.computeMargin()\n",
    "        baseline = eval2.computeMargin()\n",
    "        # print(\"Configuration {}; ConvRates {}; Margins {}; Overall Margin {}; Baseline {}\".format(pulledArm,convRates,margins,int(overall_margin*100)/100,int(baseline*100)/100))\n",
    "        # if overall_margin < baseline:\n",
    "            # print(\"VAFFANCULOOOOO {} - {} = {}\".format(overall_margin,baseline,overall_margin-baseline))\n",
    "        bf.update(overall_margin)\n",
    "\n",
    "    opt_arms.append(bf.get_optima())\n",
    "    opt_margins.append(bf.get_optima_margin())\n",
    "clear_output(wait=True)\n",
    "print(\"BRUTE FORCE OF CONFIG {} CLASSES:\".format(CONFIG_PATH))\n",
    "for i in range(0,len(opt_arms)):\n",
    "    print(\"   - [CLASS {}] Optimal arm is {} with margin {}\".format(i,opt_arms[i], opt_margins[i]))\n",
    "\n",
    "daily_users = np.array(daily_users)\n",
    "classes_weights = daily_users / daily_users.sum()\n",
    "opt_margins = np.array(opt_margins)\n",
    "print(\"\\nThe optimal weighted mean expected margin given the mean daily users {} is {}\".format(daily_users, np.multiply(classes_weights, opt_margins).sum()))\n",
    "\n",
    "# Best single arm possible\n",
    "print(\"\\nWhich arm among the best ones gives the better results in non contextual optmization?\")\n",
    "equal_arm_rew = []\n",
    "for i in range(0,len(opt_arms)):\n",
    "    arm = opt_arms[i]\n",
    "    class_rewards = []\n",
    "    for k in range(0, len(config[\"classes\"])):\n",
    "        uc = config[\"classes\"][k]\n",
    "        productList = [Product(int(key), uc[\"secondary\"][key]) for key in uc[\"secondary\"]]\n",
    "        conversionRateLevels = uc[\"conversionRates\"]\n",
    "        marginsPerPrice = config[\"margins\"]\n",
    "        click_prob = np.array(uc[\"clickProbability\"])\n",
    "        lambda_p = uc[\"lambda\"]\n",
    "        alphas = uc[\"alphas\"]\n",
    "        units_mean = uc[\"unitsShape\"]\n",
    "        num_prices = len(conversionRateLevels[0])\n",
    "        num_prods = len(alphas)\n",
    "\n",
    "        pulledArm = arm\n",
    "        margins = []\n",
    "        convRates = []\n",
    "        for k in range(0,len(pulledArm)):\n",
    "            margins.append(marginsPerPrice[k][pulledArm[k]])\n",
    "            convRates.append(conversionRateLevels[k][pulledArm[k]])\n",
    "\n",
    "        eval = GraphEvaluator(products_list=productList, click_prob_matrix=click_prob, lambda_prob=lambda_p, conversion_rates=convRates,\n",
    "                        alphas=alphas, margins=margins, units_mean=units_mean, verbose=False)\n",
    "        eval2 = Baseline(products_list=productList, click_prob_matrix=click_prob, lambda_prob=lambda_p, conversion_rates=convRates,\n",
    "                        alphas=alphas, margins=margins, units_mean=units_mean, verbose=False)\n",
    "                    \n",
    "        class_rewards.append(eval.computeMargin())\n",
    "    \n",
    "    weighted_reward = np.multiply(classes_weights, np.array(class_rewards)).sum()\n",
    "    equal_arm_rew.append(weighted_reward)\n",
    "    if class_rewards[i] < eval2.computeMargin():\n",
    "        print(\"   - [ARM {}] Class-weighted expected margin is {}, but baseline is greater than weighted\".format(arm, weighted_reward))\n",
    "    else:\n",
    "        print(\"   - [ARM {}] Class-weighted expected margin is {}\".format(arm, weighted_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average distance from Environment data and GraphEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [24:00<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE ERROR OF GRAPH EVALUATOR FROM ENVIRONMENT (./Configs/config1.json):\n",
      "   - [MEAN] 0.022692850664425425\n",
      "   - [STD] 0.01767915360945945\n",
      "   - [MAX] 0.09019686027980131\n",
      "   - [MIN] 4.097997882295597e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from Environment import Environment\n",
    "from Model.Evaluator.GraphEvaluator import GraphEvaluator\n",
    "from Model.Evaluator.Baseline import Baseline\n",
    "from Model.Evaluator.OneStepEvaluator import OneStepEvaluator\n",
    "\n",
    "from Model.Product import *\n",
    "import numpy as np\n",
    "\n",
    "# ==== PARAMETERS TO CHANGE ====\n",
    "config_path = \"./Configs/config1.json\"\n",
    "RANDOM_ARM = False\n",
    "n_experiments = 250\n",
    "# ==============================\n",
    "\n",
    "f = open(config_path)\n",
    "config = json.load(f)\n",
    "f.close()\n",
    "\n",
    "env = Environment(config_path=config_path)\n",
    "marginsPerPrice = config[\"margins\"]\n",
    "\n",
    "evaluatorEnvDifference = []\n",
    "\n",
    "bf = BruteForce(num_prices=num_prices, num_products=num_prods)\n",
    "for i in tqdm(range(0, num_prices**num_prods)):\n",
    "  arm = bf.pull_arm() \n",
    "  margins = [marginsPerPrice[i][arm[i]] for i in range(0,len(arm))]\n",
    "  # print(margins)\n",
    "  obtained_margins = []\n",
    "\n",
    "  conf_classes = config[\"classes\"]\n",
    "  for uc in conf_classes:\n",
    "      armConvRates = [uc[\"conversionRates\"][i][arm[i]] for i in range(0,len(arm))]\n",
    "      productList = [Product(int(key), uc[\"secondary\"][key]) for key in uc[\"secondary\"]]\n",
    "      eval = GraphEvaluator(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                  alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"actualUnitsMean\"], convert_units=False, verbose=False)\n",
    "      baseline = Baseline(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                  alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"actualUnitsMean\"], convert_units=False, verbose=False)\n",
    "      oneStep = OneStepEvaluator(products_list=productList, click_prob_matrix=uc[\"clickProbability\"], lambda_prob=uc[\"lambda\"], conversion_rates=armConvRates,\n",
    "                  alphas=uc[\"alphas\"], margins=margins, units_mean=uc[\"actualUnitsMean\"], verbose=False)\n",
    "\n",
    "  env.setPriceLevels(arm)\n",
    "  for i in range(0,n_experiments):\n",
    "    inters = env.round()\n",
    "    total = 0\n",
    "    for inter in inters:\n",
    "      total += inter.linearizeMargin(marginsPerPrice)\n",
    "      obtained_margins.append(inter.linearizeMargin(marginsPerPrice))\n",
    "    total = total / len(inters)\n",
    "    # obtained_margins.append(total)\n",
    "  \n",
    "  environmentMean = np.array(obtained_margins).mean()\n",
    "  # Percentage error of graph eval from environment\n",
    "  evaluatorEnvDifference.append(abs(environmentMean - eval.computeMargin()) / environmentMean)\n",
    "\n",
    "evaluatorEnvDifference = np.array(evaluatorEnvDifference)\n",
    "print(\"PERCENTAGE ERROR OF GRAPH EVALUATOR FROM ENVIRONMENT ({}):\".format(config_path))\n",
    "print(\"   - [MEAN] {}\".format(evaluatorEnvDifference.mean()))\n",
    "print(\"   - [STD] {}\".format(evaluatorEnvDifference.std()))\n",
    "print(\"   - [MAX] {}\".format(evaluatorEnvDifference.max()))\n",
    "print(\"   - [MIN] {}\".format(evaluatorEnvDifference.min()))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14160fb0de3f36180f5e1dd791af52bebcb3f3583a6e540ccb943ed2b61e8d42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
